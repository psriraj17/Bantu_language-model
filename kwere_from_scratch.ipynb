{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwnvbK9N6aN1kius4zuWUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psriraj17/Bantu_language-model/blob/bantu-ssr/kwere_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entire code in this file is extracted from 'https://github.com/gregfromstl/bantu-language-*modeling*'"
      ],
      "metadata": {
        "id": "tnMahm2ihmYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CGIOid6svmj",
        "outputId": "736ffca0-a165-4714-f295-6b360c80bf96"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XCOOLq9zqMnl"
      },
      "outputs": [],
      "source": [
        "PARAMS = {\n",
        "    'experiment_name': \"Kwere\",\n",
        "    'tags': [\"kwere\", \"from scratch\"],\n",
        "    'n': 1000,\n",
        "    'threshold': 750,\n",
        "    'train_iterations': 10,\n",
        "    'carry_hidden_state': False,\n",
        "    'val_split': 0.2,\n",
        "    'kwere_train': \"/content/drive/MyDrive/Colab Notebooks/train-04/cwe-train.txt\",\n",
        "    'pretrain_iterations': 5,\n",
        "    'pretrain_percentage': 0.01, \n",
        "    'swahili': \"/content/drive/MyDrive/Colab Notebooks/train-04/sw-train.txt\",\n",
        "    'test_data': \"/content/drive/MyDrive/test04/cwe-test.txt\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n"
      ],
      "metadata": {
        "id": "LfjxfNqurzDQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    def __init__(self, raw_data: str):\n",
        "        self.chars = set(list(set(raw_data)))\n",
        "        self.chars.add('~')\n",
        "        self.data_size, self.vocab_size = len(raw_data), len(self.chars)\n",
        "        print(\"{} characters, {} unique\".format(self.data_size, self.vocab_size))\n",
        "        \n",
        "        self.char_to_idx = { char: idx for idx, char in enumerate(self.chars) }\n",
        "        self.idx_to_char = { idx: char for idx, char in enumerate(self.chars) }\n",
        "        self.data = [self.char_to_idx[char] for char in list(raw_data)]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]"
      ],
      "metadata": {
        "id": "B1MDSikfry0r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(raw_data: str, known_chars: str) -> str:\n",
        "    cleaned = \"\"\n",
        "    for char in raw_data:\n",
        "        if char not in known_chars:\n",
        "            cleaned += '~'\n",
        "        else:\n",
        "            cleaned += char\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "JBk5PLOUryza"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Kwere training data:\", end=\"\\n\\t\")\n",
        "raw_kwere = open(PARAMS['kwere_train'], 'r').read()\n",
        "kwere_train_size, kwere_val_size = int(len(raw_kwere)*(1-PARAMS['val_split'])), int(len(raw_kwere)*PARAMS['val_split'])\n",
        "\n",
        "train_data = Dataset(raw_kwere[:kwere_train_size])\n",
        "\n",
        "print(\"Loading Kwere validation data:\", end=\"\\n\\t\")\n",
        "cleaned_kwere_val_data = clean_data(raw_kwere[kwere_train_size:], train_data.chars)\n",
        "val_data = Dataset(cleaned_kwere_val_data)\n",
        "\n",
        "\n",
        "if PARAMS['pretrain_percentage'] > 0:\n",
        "    print(\"Loading Swahili data:\", end=\"\\n\\t\")\n",
        "    raw_swahili = open(PARAMS['swahili'], 'r').read()\n",
        "    swahili_size = int(len(raw_swahili) * PARAMS['pretrain_percentage'])\n",
        "\n",
        "    cleaned_swahili_data = clean_data(raw_swahili[:swahili_size], train_data.chars)\n",
        "    pretrain_data = Dataset(cleaned_swahili_data)\n",
        "\n",
        "\n",
        "if len(PARAMS['test_data']) > 0:\n",
        "\n",
        "    print(\"Loading Testing data:\", end=\"\\n\\t\")\n",
        "    raw_test = open(PARAMS['test_data'], 'r').read()\n",
        "\n",
        "    cleaned_test_data = clean_data(raw_test, train_data.chars)\n",
        "    test_data = Dataset(cleaned_test_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny-0okaoryxC",
        "outputId": "9a14f844-ba65-4f26-cce4-de10d343e595"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Kwere training data:\n",
            "\t482745 characters, 32 unique\n",
            "Loading Kwere validation data:\n",
            "\t120687 characters, 32 unique\n",
            "Loading Swahili data:\n",
            "\t392610 characters, 32 unique\n",
            "Loading Testing data:\n",
            "\t61717 characters, 32 unique\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CountMatrix:\n",
        "    def __init__(self, vocab: list, init_matrix=None):\n",
        "        self.counts = init_matrix if init_matrix is not None else {i:0 for i in vocab}\n",
        "        self.next = {i:None for i in vocab}\n",
        "        "
      ],
      "metadata": {
        "id": "U_ELewecryua"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increment_count(char: str, sequence: list, count_matrix: CountMatrix) -> list:\n",
        "    next_char = sequence[-1]\n",
        "    \n",
        "    count_matrix.counts[char] += 1\n",
        "    if count_matrix.next[next_char] is not None:\n",
        "        count_matrix.next[next_char] = increment_count(char, sequence[:-1], count_matrix.next[next_char])\n",
        "    elif sum(count_matrix.counts.values()) > PARAMS['threshold']:\n",
        "        vocab = count_matrix.next.keys()\n",
        "        initial_matrix = {i:0 for i in vocab}\n",
        "        initial_matrix[char] += 1\n",
        "        count_matrix.next = {i:CountMatrix(vocab, initial_matrix) for i in vocab}\n",
        "    \n",
        "    return count_matrix"
      ],
      "metadata": {
        "id": "2BAqIBoVryr0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_counts(data: Dataset, n: int, count_matrix: CountMatrix):\n",
        "    for idx, char in enumerate(data[n:]):\n",
        "        idx = n + idx\n",
        "        sequence = data[idx-n:idx]\n",
        "        \n",
        "        count_matrix = increment_count(data[idx], sequence, count_matrix)\n",
        "    return count_matrix"
      ],
      "metadata": {
        "id": "qxdMZ8b7ryn8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building Matrix...\")\n",
        "count_matrix = CountMatrix(vocab=train_data.idx_to_char.keys())\n",
        "\n",
        "print(\"Fitting...\")\n",
        "if PARAMS['pretrain_percentage'] > 0:\n",
        "    for i in range(PARAMS['pretrain_iterations']):\n",
        "        count_matrix = iterate_counts(pretrain_data, PARAMS['n'], count_matrix)\n",
        "\n",
        "for i in range(PARAMS['train_iterations']):\n",
        "    count_matrix = iterate_counts(train_data, PARAMS['n'], count_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RecEv-3hsC-h",
        "outputId": "88d48a11-f2e2-4e15-fd79-56eb67095889"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Matrix...\n",
            "Fitting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def probabilities_from_counts(counts: dict):\n",
        "    # add one smoothing\n",
        "    counts = {key:counts[key]+1 for key in counts.keys()}\n",
        "    \n",
        "    probabilities = {key: counts[key] / sum(counts.values()) for key in counts.keys()}\n",
        "    prob_sum = sum(probabilities.values())\n",
        "    assert(abs(prob_sum - 1) < 0.0001), \"Probabilities should sum to 1.0 but got {}\".format(prob_sum)\n",
        "    \n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "f8vscIaUsC7q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probabilities_for_sequence(sequence: list, count_matrix: CountMatrix):\n",
        "    # return counts if sequence has been exhausted\n",
        "    if len(sequence) == 0:\n",
        "        return count_matrix.counts\n",
        "    \n",
        "    next_char = sequence[-1]\n",
        "    \n",
        "    if count_matrix.next[next_char] is not None:\n",
        "        return get_probabilities_for_sequence(sequence[:-1], count_matrix.next[next_char])\n",
        "    else:\n",
        "        return probabilities_from_counts(count_matrix.counts)"
      ],
      "metadata": {
        "id": "LLw77AQ9sIzg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(target_prob):\n",
        "    return -math.log(target_prob, 2)"
      ],
      "metadata": {
        "id": "a9PP4TBfsItU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(data: Dataset, n: int, count_matrix: CountMatrix):\n",
        "    print(\"Evaluating...\")\n",
        "    \n",
        "    counter = 0\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    \n",
        "    for idx, char in enumerate(data[n:]):\n",
        "        idx = n + idx\n",
        "        sequence = data[idx-n:idx]\n",
        "\n",
        "        probabilities: dict = get_probabilities_for_sequence(sequence, count_matrix)\n",
        "        pred: str = max(probabilities, key=probabilities.get)\n",
        "        target: str = data[idx]\n",
        "        target_prob: float = probabilities[target]\n",
        "        \n",
        "        running_loss += calc_loss(target_prob)\n",
        "        running_acc += 1 if target == pred else 0\n",
        "        counter += 1\n",
        "        \n",
        "    return running_loss / counter, running_acc / counter"
      ],
      "metadata": {
        "id": "4PUme8CisMyZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_acc = eval(train_data, PARAMS['n'], count_matrix)\n",
        "print(\"Train Loss: {:.3f}\\t\\t|\\tTrain Accuracy: {:.2f}%\".format(train_loss, train_acc*100))\n",
        "\n",
        "val_loss, val_acc = eval(val_data, PARAMS['n'], count_matrix)\n",
        "print(\"Validation Loss: {:.3f}\\t\\t|\\tValidation Accuracy: {:.2f}%\".format(val_loss, val_acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAcmpfGpsMu2",
        "outputId": "a7edb081-9be5-40d7-c186-956c8a2c3add"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n",
            "Train Loss: 1.612\t\t|\tTrain Accuracy: 68.03%\n",
            "Evaluating...\n",
            "Validation Loss: 6.656\t\t|\tValidation Accuracy: 9.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(PARAMS['test_data']) > 0:\n",
        "    test_loss, test_acc = eval(test_data, PARAMS['n'], count_matrix)\n",
        "    print(\"Test Loss: {:.3f}\\t\\t|\\tTest Accuracy: {:.2f}%\".format(test_loss, test_acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC1qgzDYiBlE",
        "outputId": "bb7bfb52-cdb9-440c-8b0a-81a1148e9cbc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n",
            "Test Loss: 3.057\t\t|\tTest Accuracy: 47.76%\n"
          ]
        }
      ]
    }
  ]
}